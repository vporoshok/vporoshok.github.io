---
title: Слияние больших массивов данных
description: На нашем кассетнике кончилась плёнка — мотай!
date: 2018-11-08T14:00:00Z
draft: false
categories:
- algorithm
tags:
- data-structure
- memory
- complexity
toc: true
katex: true
---

Итак, изучив кучи в первом приближении, можно вернуться к сортировке слиянием и рассмотреть те случаи, где другая сортировка работать не будет в принципе. Речь на этот раз пойдёт о таких объёмах данных, которые не влезают в оперативную (внутреннюю) память.

## Об уровнях памяти

Часто, говоря о памяти компьютера, предлагают рассматривать лишь оперативную (RAM) и дисковую (HDD / SSD). На самом деле уровней памяти в современном компьютере значительно больше. Если упростить некоторые вещи, то можно рассматривать следующую иерархию:
1. регистры процессора: сотни байт, единицы тактов;
2. кеши процессора: от десятков килобайт до нескольких мегабайт, от нескольких до сотен тактов;
3. оперативная память: десятки и сотни гигабайт, сотни и тысячи тактов;
4. дисковое хранилище: терабайты, миллионы тактов;
5. ленточные библиотеки: неограниченно, секунды или даже минуты.

Чаще всего рассматривается не абсолютное время, а время в циклах или тактах процессора, таким образом можно понять насколько проседает производительность компьютера во время обращения к той или иной системе памяти.

### Эволюция и скорость

<table>
<tr>
<th>
<th scope="col">1980 VAX-11/750 (1980)
<th scope="col">Домашний компьютер (2007)
<th scope="col">Улучшение
<tr>
<th scope="row">Частота (МГц)
<td>6
<td>3000
<td style="color:green">&times;500
<tr>
<th scope="row">Размер памяти (Мб)
<td>2
<td>2000
<td style="color:green">&times;1000
<tr>
<th scope="row">Скорость чтения (Мб/с)
<td>13
<td>7000 (read)<br>2000 (write)
<td style="color:green">&times;540<br>&times;150
<tr>
<th scope="row">Время отклика (нс)
<td>225
<td>~70
<td>&times;3
<tr>
<th scope="row">Время отклика (такты)
<td>1.4
<td>210
<td style="color:red">&divide;150
<caption>
Сравнение производительности элементов компьютера между 1980 и 2007 гг.[^1]
</caption>
</table>
С эволюцией компьютеров развиваются и системы памяти. Увеличиваются объёмы, пропускные способности шин. Только один показатель растёт (точнее уменьшается) недостаточно быстро: время отклика памяти. Если по сравнению с компьютером 80-го года частота процессора выросла к 2007 году в 500 раз, пропускная способность шины памяти — более чем в 500 раз на чтение и в 150 раз на запись, время отклика уменьшилось лишь в 3 раза. Таким образом, пока обрабатывается запрос до памяти процессор стал пропускать в 150 раз больше полезных тактов, чем в 80-м году. Конечно, это спекуляция на числах, однако, помнить об этом надо.

Что же означает: пропускная спопобность шины выросла более чем в 500 раз, а время отклика — всего в 3. Это означает, что мы увеличили «диаметр трубы», но длинная её осталась практически прежней. Из этого соотношения вырастает правило, действующее на всех уровнях иерархии памяти: читай с запасом.

### Читай с запасом

Современные диски не дают возможность прочитать один байт. На уровне встроенных микроконтроллеров они возвращают сектор размером 512 байт или 4 Кб. Если же довериться операционной системе, то мы получим размер кластера от 4 Кб до 64 Кб. На самом деле эффективное чтение (то есть когда время отклика становится меньше времени чтения) начинается от нескольких мегабайт.

{{<note "info">}}
Если верить сводкам с фронта, для современных SSD время «позиционирования» составляет 0.08–0.16 мс, а скорость чтения упирается в скорость интерфейса подключения, например, для SATA-3 это 6 Гб/с. Таким образом за 0.08 мс будет прочитано примерно 75 Мб. Кончено, максимальная скорость развивается в том случае, если файл не фрагментирован, да и то не сразу, так что можем спокойно поделить результат пополам. Итого получаем 38 Мб информации прочитается за то же время, что будет пауза между обращением и непосредственно чтением.
{{</note>}}

А таблица выше говорит о том, что подобная ситуация в современных компьютерах стала актуальна не только для чтения с диска, но и для работы с оперативной памятью. Так, например, процессор не взаимодействует с памятью напрямую. Если необходимо прочитать значение переменной, целый кусок памяти считывается в кеш процессора. Затем, после выполнения операций, весь кусок записывается обратно в память.

Это накладывает определённые требования на эффективные алгоритмы, например локальность данных или кеш-независимость (cache-oblivious)[^2]. В случае с сортировками слияния оба требования выполняются за счёт последовательных чтения и записи.

## Применение

Рассматриваемые оптимизации алгоритмов в первую очередь применимы при работе с дисковыми хранилищами, но, если присмотреться, то возможны улучшения и на более верхних уровнях памяти как кеш-независимые алгоритмы. Сортировки таких больших объёмов данных применяются в базах данных как для хранения данных (например, один из самых популярных дисковых бэкендов современных бд LevelDB очень активно использует слияние файлов в переходах между уровнями[^3]), так и для получения данных с сортировкой.

Ниже мы будем рассматривать задачу сортировки \\(n\\) ключей на машине, где в оперативную память помещается лишь \\(m\\) ключей. Причём будем считать, что \\(m \ll n\\). В целом аналогичную задачу можно рассматривать и для пары оперативная память / кеш процессора.

## План решения задачи целиком

Итак, у нас есть файл, содержащий \\(n\\) ключей, при этом в оперативную память помешается только \\(m\\). Поступим следующим образом:
- разделим файл на части размером \\(m\\);
- отсортируем каждую часть в оперативной памяти;
- будем сливать части до тех пор, пока не получим отсортированным весь объём ключей.

Первые два пункта понятны, поэтому займёмся последним.

Итак, пусть \\(k = n / m \gg 100\\). Каким образом будем сливать получившиеся части? Первое, о чём надо помнить: надо как можно меньше читать и писать. Именно чтение и запись являются бутылочным горлышком всей задачи. Таким образом сливать попарно кусочки будет не лучшей затеей, потому как каждый ключ мы прочитаем с диска и запишем обратно \\(\log(n/m)\\) раз. Чтобы уменьшить высоту дерева слияния надо увеличить основание логарифма. Для этого надо будет сливать не по паре частей, а сразу по 10, 20 или… Сколько кусочков надо сливать?

Для ответа на этот вопрос надо вспомнить правило _читай с запасом_. Итак, ни чтение, ни запись не происходит по одному элементу, для обоих операций используется некоторый буфер. Размер этого буфера выбирается по правилу соотношения с времени позиционирования и пропускной способности шины. Пусть размер буфера будет составлять \\(b\\) ключей. Тогда сливать мы сможем не более чем
\\[
p = \frac{m}{b} - 1
\\]
частей (один буфер будет использоваться для записи результата). Например, пусть минимально эффективный размер буфера для диска составляет 1 Мб, а объём выделенной для процесса памяти равен 1 Гб, тогда надо будет сливать 1023 части. Как же выбирать следующий минимум из такого количества буферов?

## Добавим немного куч

Для решения этой задачи отлично подходит [известная нам структура кучи](/post/2018/11/simple-heap/). В итоге получаем следующий алгоритм слияния:
- вычитываем в память буферы \\(p\\) частей;
- образуем из вычитанных ключей кучу;
- выбираем минимум из кучи и пишем его в буфер записи;
- как только буфер записи заполнен, пишем его на диск и очищаем.

Но тут возникает следующий вопрос: когда читать следующие блоки из файлов источников? Вычитывать их надо ровно тогда, когда иссякает их буфер чтения. Однако элементы уже переставлены из буферов чтения так, что концов не сыщешь. Для этого необходимо помечать последние элементы вычитанных блоков. Для этого требуется поддерживать \\(p\\) итераторов. Так как итераторы нужны не для всех элементов, а только для \\(p\\) штук, лучше всего здесь подойдёт хеш-таблица, сопоставляющая последним в блоках ключам номера блоков. И каждый раз, когда выбирается минимум, делать проверку — не является ли этот ключ последним в каком-либо из блоков. Есть нюансы с одинаковыми ключами, но их можно обойти, хешируя не ключи, а значения или меняя размер блоков.

## Немного больше

Но можно улучшить схему ещё больше. Если вдруг так оказалось, что одна из сливаемых частей сильно меньше остальных (поэлементно) и закончилась (была полностью вычитана) раньше остальных, то можно начать читать следующую часть. При этом возможно 2 варианта:
1. минимальный элемент новой части больше текущего минимума кучи;
2. минимальный элемент новой части меньше или равен текущего минимума.

В первом случае нам повезло и мы можем продолжать читать блоки новой части так, как если бы она с самого начала участвовала в слиянии. Во втором случае необходимо отделить этот блок в новую кучу. В следующий раз можно вычитать блок из другой части и вновь сравнить его с головой текущей кучи. В итоге, когда текущая куча будет полностью вычитана, новая куча будет уже содержать вычитанные \\(p\\) блоков из \\(p\\) ещё не слитых частей, необходимо будет только писать в новый файл. Таким образом результирующий файл получается в среднем размером не \\(pm\\), а в \\(2\\) раза больше[^4].

## Задания

1. Реализуйте слияние \\(k\\) массивов с использованием двоичной кучи.

## Дополнительное чтение

1. Максим Александрович Бабенко. _Видеолекции курса «Алгоритмы и структуры данных»_. Школа Анализа Данных. https://yandexdataschool.ru/edu-process/courses/algorithms#item-3
2. Георгий Кириченко. _Видеолекции курса «Базы данных»_. Техносфера Mail.ru Group. https://www.youtube.com/playlist?list=PLrCZzMib1e9o-2km1HniylB-ZZteznvLb

[^1]: Herb Sutter. "Machine Architecture: Things Your Programming Language Never Told You." _Northwest C++ Users' Group_. 2007. http://nwcpp.org/talks/2007/Machine_Architecture_-_NWCPP.pdf
[^2]: Hiroshi Inoue, Kenjiro Taura. "SIMD- and Cache-Friendly Algorithm for Sorting an Array of Structures." _Proceedings of the VLDB Endowment_. vol. 8, iss. 11, 2015, pp. 1274-1285. http://www.vldb.org/pvldb/vol8/p1274-inoue.pdf
[^3]: Patrick O'Neil1 and al. "The Log-Structured Merge-Tree (LSM-Tree)." _Acta Informatica_. vol. 33, no. 4, 1996, pp. 351–385. https://www.cs.umb.edu/~poneil/lsmtree.pdf
[^4]: Xavier Martinez-Palau, David Dominguez-Sal, Josep Lluis Larriba-Pey. "Two-way Replacement Selection." _Proceedings of the VLDB Endowment VLDB Endowment_. vol. 3, iss. 1-2, 2010, pp. 871-881. http://www.vldb.org/pvldb/vldb2010/papers/R78.pdf
